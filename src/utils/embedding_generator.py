"""
ì„ë² ë”© ìƒì„± ëª¨ë“ˆ - HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
ëª¨ë¸: sentence-transformers/paraphrase-multilingual-mpnet-base-v2 (768ì°¨ì›)
"""
import torch
from transformers import AutoTokenizer, AutoModel
from typing import List, Optional
from config import EMBEDDING_CONFIG


class EmbeddingGenerator:
    """
    HuggingFace ëª¨ë¸ì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±ê¸°
    """

    def __init__(self, model_name: str = None, device: str = None):
        """
        ì„ë² ë”© ìƒì„±ê¸° ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ëª… (ê¸°ë³¸: config.py ì„¤ì •ê°’)
            device: ì—°ì‚° ì¥ì¹˜ ('cuda', 'cpu', ë˜ëŠ” None=ìë™ ê°ì§€)
        """
        self.model_name = model_name or EMBEDDING_CONFIG.get(
            'hf_model',
            'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
        )

        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device is None:
            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        else:
            self.device = device

        print(f"ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_name}")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")

        # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModel.from_pretrained(self.model_name)
        self.model.to(self.device)
        self.model.eval()  # ì¶”ë¡  ëª¨ë“œ

        print(f"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ì°¨ì›: {self.get_dimension()})")

    def get_dimension(self) -> int:
        """ì„ë² ë”© ì°¨ì› ìˆ˜ ë°˜í™˜"""
        return self.model.config.hidden_size

    def _mean_pooling(self, model_output, attention_mask) -> torch.Tensor:
        """
        Mean Pooling - attention maskë¥¼ ê³ ë ¤í•œ í‰ê·  ê³„ì‚°
        """
        token_embeddings = model_output[0]  # ëª¨ë“  í† í° ì„ë² ë”©
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(
            input_mask_expanded.sum(1), min=1e-9
        )

    def embed_text(self, text: str) -> List[float]:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±

        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸

        Returns:
            List[float]: ì„ë² ë”© ë²¡í„°
        """
        return self.embed_texts([text])[0]

    def embed_texts(self, texts: List[str], batch_size: int = 32) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ìƒì„±

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°

        Returns:
            List[List[float]]: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i + batch_size]

            # í† í°í™”
            encoded_input = self.tokenizer(
                batch_texts,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors='pt'
            )
            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

            # ì„ë² ë”© ìƒì„±
            with torch.no_grad():
                model_output = self.model(**encoded_input)

            # Mean pooling
            embeddings = self._mean_pooling(model_output, encoded_input['attention_mask'])

            # ì •ê·œí™” (ì„ íƒì ì´ì§€ë§Œ ìœ ì‚¬ë„ ê²€ìƒ‰ì— ìœ ìš©)
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

            # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
            embeddings = embeddings.cpu().tolist()
            all_embeddings.extend(embeddings)

        return all_embeddings

